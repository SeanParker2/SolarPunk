#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®å¤‡ä»½è„šæœ¬
ç”¨äºå¤‡ä»½SQLiteæ•°æ®åº“å’Œç”Ÿæˆå¤‡ä»½æŠ¥å‘Š
"""

import os
import shutil
import sqlite3
import json
from datetime import datetime
from pathlib import Path
import zipfile
import argparse

def get_database_path():
    """è·å–æ•°æ®åº“è·¯å¾„"""
    # ä»ç¯å¢ƒå˜é‡æˆ–é»˜è®¤è·¯å¾„è·å–æ•°æ®åº“ä½ç½®
    db_url = os.getenv('DATABASE_URL', 'sqlite:///./solarpunk.db')
    if db_url.startswith('sqlite:///'):
        return db_url.replace('sqlite:///', '')
    else:
        print("âš ï¸  å½“å‰é…ç½®ä½¿ç”¨çš„ä¸æ˜¯SQLiteæ•°æ®åº“")
        print(f"   æ•°æ®åº“URL: {db_url}")
        print("   æ­¤è„šæœ¬ä»…æ”¯æŒSQLiteæ•°æ®åº“å¤‡ä»½")
        return None

def create_backup_directory():
    """åˆ›å»ºå¤‡ä»½ç›®å½•"""
    backup_dir = Path('backups')
    backup_dir.mkdir(exist_ok=True)
    return backup_dir

def backup_database(db_path, backup_dir):
    """å¤‡ä»½æ•°æ®åº“æ–‡ä»¶"""
    if not Path(db_path).exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_filename = f"solarpunk_db_backup_{timestamp}.db"
    backup_path = backup_dir / backup_filename
    
    try:
        shutil.copy2(db_path, backup_path)
        print(f"âœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_path}")
        return backup_path
    except Exception as e:
        print(f"âŒ æ•°æ®åº“å¤‡ä»½å¤±è´¥: {e}")
        return None

def export_data_to_json(db_path, backup_dir):
    """å¯¼å‡ºæ•°æ®ä¸ºJSONæ ¼å¼"""
    if not Path(db_path).exists():
        return None
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    json_filename = f"solarpunk_data_export_{timestamp}.json"
    json_path = backup_dir / json_filename
    
    try:
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row  # ä½¿ç»“æœå¯ä»¥æŒ‰åˆ—åè®¿é—®
        
        export_data = {
            'export_time': datetime.now().isoformat(),
            'tables': {}
        }
        
        # è·å–æ‰€æœ‰è¡¨å
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'alembic_%'")
        tables = [row[0] for row in cursor.fetchall()]
        
        for table in tables:
            cursor = conn.execute(f"SELECT * FROM {table}")
            rows = cursor.fetchall()
            export_data['tables'][table] = [dict(row) for row in rows]
            print(f"   å¯¼å‡ºè¡¨ {table}: {len(rows)} æ¡è®°å½•")
        
        conn.close()
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ: {json_path}")
        return json_path
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
        return None

def create_backup_archive(backup_files, backup_dir):
    """åˆ›å»ºå¤‡ä»½å‹ç¼©åŒ…"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    archive_filename = f"solarpunk_backup_{timestamp}.zip"
    archive_path = backup_dir / archive_filename
    
    try:
        with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_files:
                if file_path and Path(file_path).exists():
                    zipf.write(file_path, Path(file_path).name)
        
        print(f"âœ… å¤‡ä»½å‹ç¼©åŒ…åˆ›å»ºæˆåŠŸ: {archive_path}")
        return archive_path
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤‡ä»½å‹ç¼©åŒ…å¤±è´¥: {e}")
        return None

def generate_backup_report(db_path, backup_files):
    """ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š"""
    print("\nğŸ“Š å¤‡ä»½æŠ¥å‘Š")
    print("=" * 50)
    
    if not Path(db_path).exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        conn = sqlite3.connect(db_path)
        
        # ç»Ÿè®¡å„è¡¨è®°å½•æ•°
        cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'alembic_%'")
        tables = [row[0] for row in cursor.fetchall()]
        
        print("ğŸ“‹ æ•°æ®åº“ç»Ÿè®¡:")
        total_records = 0
        for table in tables:
            cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            total_records += count
            print(f"   â€¢ {table}: {count} æ¡è®°å½•")
        
        print(f"   æ€»è®¡: {total_records} æ¡è®°å½•")
        
        # æ•°æ®åº“æ–‡ä»¶å¤§å°
        db_size = Path(db_path).stat().st_size
        print(f"   æ•°æ®åº“å¤§å°: {db_size / 1024 / 1024:.2f} MB")
        
        conn.close()
        
        # å¤‡ä»½æ–‡ä»¶ä¿¡æ¯
        print("\nğŸ’¾ å¤‡ä»½æ–‡ä»¶:")
        for file_path in backup_files:
            if file_path and Path(file_path).exists():
                size = Path(file_path).stat().st_size
                print(f"   â€¢ {Path(file_path).name}: {size / 1024 / 1024:.2f} MB")
        
    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

def cleanup_old_backups(backup_dir, keep_days=7):
    """æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶"""
    print(f"\nğŸ§¹ æ¸…ç†{keep_days}å¤©å‰çš„å¤‡ä»½æ–‡ä»¶...")
    
    cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 60 * 60)
    deleted_count = 0
    
    for file_path in backup_dir.glob('*'):
        if file_path.is_file() and file_path.stat().st_mtime < cutoff_time:
            try:
                file_path.unlink()
                print(f"   åˆ é™¤: {file_path.name}")
                deleted_count += 1
            except Exception as e:
                print(f"   åˆ é™¤å¤±è´¥ {file_path.name}: {e}")
    
    if deleted_count == 0:
        print("   æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ–‡ä»¶")
    else:
        print(f"   å…±åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶")

def show_backup_recommendations():
    """æ˜¾ç¤ºå¤‡ä»½å»ºè®®"""
    print("\nğŸ’¡ æ•°æ®å¤‡ä»½æœ€ä½³å®è·µ")
    print("=" * 50)
    print("1. ğŸ”„ è‡ªåŠ¨å¤‡ä»½è®¾ç½®:")
    print("   â€¢ è®¾ç½®æ¯æ—¥è‡ªåŠ¨å¤‡ä»½(æ¨èå‡Œæ™¨æ‰§è¡Œ)")
    print("   â€¢ ä½¿ç”¨cron jobæˆ–éƒ¨ç½²å¹³å°çš„å®šæ—¶ä»»åŠ¡")
    print("   â€¢ ç¤ºä¾‹cron: 0 2 * * * /path/to/backup_script.py")
    
    print("\n2. ğŸŒ äº‘ç«¯å¤‡ä»½:")
    print("   â€¢ å°†å¤‡ä»½æ–‡ä»¶ä¸Šä¼ åˆ°äº‘å­˜å‚¨(AWS S3, Google Driveç­‰)")
    print("   â€¢ ä½¿ç”¨æ•°æ®åº“æ‰˜ç®¡æœåŠ¡çš„è‡ªåŠ¨å¤‡ä»½åŠŸèƒ½")
    print("   â€¢ è€ƒè™‘è·¨åœ°åŸŸå¤‡ä»½ä»¥é˜²ç¾éš¾æ¢å¤")
    
    print("\n3. ğŸ” å¤‡ä»½å®‰å…¨:")
    print("   â€¢ åŠ å¯†æ•æ„Ÿå¤‡ä»½æ–‡ä»¶")
    print("   â€¢ é™åˆ¶å¤‡ä»½æ–‡ä»¶è®¿é—®æƒé™")
    print("   â€¢ å®šæœŸæµ‹è¯•å¤‡ä»½æ¢å¤æµç¨‹")
    
    print("\n4. ğŸ“… å¤‡ä»½ç­–ç•¥:")
    print("   â€¢ æ¯æ—¥å¤‡ä»½ä¿ç•™7å¤©")
    print("   â€¢ æ¯å‘¨å¤‡ä»½ä¿ç•™4å‘¨")
    print("   â€¢ æ¯æœˆå¤‡ä»½ä¿ç•™12ä¸ªæœˆ")
    print("   â€¢ é‡è¦èŠ‚ç‚¹æ‰‹åŠ¨å¤‡ä»½")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Solarpunk Gallery æ•°æ®å¤‡ä»½å·¥å…·')
    parser.add_argument('--cleanup-days', type=int, default=7, help='æ¸…ç†å¤šå°‘å¤©å‰çš„å¤‡ä»½æ–‡ä»¶(é»˜è®¤7å¤©)')
    parser.add_argument('--no-cleanup', action='store_true', help='è·³è¿‡æ¸…ç†æ—§å¤‡ä»½')
    parser.add_argument('--json-only', action='store_true', help='ä»…å¯¼å‡ºJSONæ ¼å¼')
    parser.add_argument('--db-only', action='store_true', help='ä»…å¤‡ä»½æ•°æ®åº“æ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ’¾ Solarpunk Gallery æ•°æ®å¤‡ä»½å·¥å…·")
    print("=" * 50)
    
    # è·å–æ•°æ®åº“è·¯å¾„
    db_path = get_database_path()
    if not db_path:
        return
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    backup_dir = create_backup_directory()
    print(f"ğŸ“ å¤‡ä»½ç›®å½•: {backup_dir.absolute()}")
    
    backup_files = []
    
    # æ‰§è¡Œå¤‡ä»½
    if not args.json_only:
        print("\nğŸ”„ å¤‡ä»½æ•°æ®åº“æ–‡ä»¶...")
        db_backup = backup_database(db_path, backup_dir)
        if db_backup:
            backup_files.append(db_backup)
    
    if not args.db_only:
        print("\nğŸ“¤ å¯¼å‡ºæ•°æ®ä¸ºJSON...")
        json_backup = export_data_to_json(db_path, backup_dir)
        if json_backup:
            backup_files.append(json_backup)
    
    # åˆ›å»ºå‹ç¼©åŒ…
    if backup_files:
        print("\nğŸ“¦ åˆ›å»ºå¤‡ä»½å‹ç¼©åŒ…...")
        archive = create_backup_archive(backup_files, backup_dir)
        if archive:
            backup_files.append(archive)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_backup_report(db_path, backup_files)
    
    # æ¸…ç†æ—§å¤‡ä»½
    if not args.no_cleanup:
        cleanup_old_backups(backup_dir, args.cleanup_days)
    
    # æ˜¾ç¤ºå»ºè®®
    show_backup_recommendations()
    
    print("\nâœ¨ å¤‡ä»½å®Œæˆ!")

if __name__ == "__main__":
    main()